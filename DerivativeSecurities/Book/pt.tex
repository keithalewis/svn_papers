\chapter{Probability Theory}

\section{Sample Spaces}
\label{pt-ss}
The possible outcomes of a random trial are described by a set, $\Omega$, called the
{\em sample space}. Knowing the outcome of the trial is equivalent to knowing
which element, $\omega\in\Omega$, occured.

\subsection{Partitions}
A {\em partion} of a set is a collection of pairwise disjoint
subsets having union equal to the set. Partitions are used to
represent partial information about an outcome.
The trivial partition consisting of the entire set represents
having no information, and the partition consisting of all
singleton sets represents complete information about the outcome.

\subsection{Algebras}
An {\em algebra} of sets is a collection of subsets
that is closed under union and complement. 
An {\em atom} of an algebra is an element of the algebra having no
proper subset in the algebra.
If the algebra is finite, there is a one-to-one correspondence between algebras and partitions:
the partition is the collection of atoms and
the algebra is the smallest algebra containing the atoms.
A {\em $\sigma$-algeba} is an algebra closed under countable unions. This class of
algebras is natural to introduce when considering
limits of random variables. The $\sigma$-algebra
generated by the open subsets of a topological space is called a {\em Borel  $\sigma$-algebra}
and its elements are called Borel sets.

Note that algebras of sets correspond to algebras of functions. The set $A\in\F$
corresponds to the {\em characteristic function} $1_E$ on $\Omega$ defined by
$1_A(\omega) = 1$ if $\omega\in A$ and $= 0$ if $\omega\not\in A$. The correspondence between
set theoretic operations and algebraic operations is $1_{\Omega\setminus A} = 1_\Omega - 1_A$ and
$1_{A\cup B} = 1_A + 1_B - 1_{A\cap F}$.

\section{Probability Spaces}
A {\em measure} is a function, $\mu$, from an algebra, $\F$, on $\Omega$
to the real numbers with the property  
$\mu(\cup_j A_j) = \sum_j \mu(A_j)$ if the $A_j\in F$ are pairwise disjoint.
If $\F$ is a $\sigma$-algebra, the sum is countably infinite. If 
$\mu(A) \ge0$ for all $A\in\F$, then $\mu$ is a {\em positive measure}.
A {\em probability measure}, $P$, is a positve measure such that $P(\Omega) = 1$.

\subsection{Measurable Functions}
A function, $X\colon\Omega\to\R$, is {\em measurable} if every preimage
of an open set belongs to the algebra of the probability space.
$X^{-1}(O) = \{\omega\in\Omega:X(\omega)\in O\}\in\F$ for every open set $O\subset\R$.
Note $X^{-1}(A)\in\F$ for every Borel set $A\subset\R$.

The {\em expected value} of $X$ is $E[X] = \int_\Omega X\,dP$. In particular
$E[1_A] = P(A)$ for $A\in\F$.

Let $L^p(\Omega, P, \F)$ be the space of $\F$ measurable functions with
the norm $||X||_p = E[|X|^p]^{1/p}$ for $1\le p < \infty$ and
$||X||_\infty = \inf \{M\in\R : P(|X| > M) = 0\}$. Recall the dual spaces
are $L^p(P)^* = L^q(P)$, for $1 \le p < \infty$, where $1/p + 1/q = 1$. In
particular, $L^2(P)$ is a Hilbert space with inner product
$(X, Y) = E[XY]$.

\subsection{Conditional Expectation}
For two events $A$, $B$, the probability of $A$ given $B$ is defined to
be $P(A\cap B)/P(B)$. This suggests we define $E[1_A|B] = \int_B 1_A\,dP/P(B)$.
If the algebra, $\G$, is finite, define $E[X|\F] = \sum_{B_j} \int_{B_j} X\,dP/P(B_j) 1_{B_j}$,
where $B_j$ are the atoms of $\G$.
More generally,
given a $\sigma$-subalgebra, $\G\subset\F$,
the conditional expectation of the measurable function $X$ given $\G$,
$E[X|\G]$ is defined to be the unique function $Y$ that is measurable
with respect to $\G$ such that $\int_A Y\,dP = \int_A X\,dP$ for all
$A\in\G$, the Radon-Nikodym derivative of $P|_\G$ with respect to $P$.
If we restrict conditional expectation to the square integrable functions
then it is the orthogonal projection from $L^2(\Omega, P, \F)$ onto $L^2(\Omega, P, \G)$.

If $X$ is $\G$ measurable, then $E[XY|\G] = X E[Y|\G]$. Also, if 
$\F\supset\G\supset\H$, then $E[E[X|\G]|\H] = E[X|\H]$.

\section{Random Variables}
A random variable, $X$, is completely determined by its {\em cumulative distribution
function}, $F(x) = Prob(X\le x)$ defined on the real line. The c.d.f. satisfies
\begin{enumerate}
\item $\lim_{x\to-\infty} F(x) = 0$ and $\lim_{x\to+\infty} F(x) = 1$
\item $F(x) \le F(x')$ for $x < x'$.
\item $F(x)$ is right continuous.
\end{enumerate}
Conversely, any function satisfying these conditions is the c.d.f. of
a random variable.
The c.d.f. provides sufficient
information to compute $Prob(X\in A)$ for any Borel set $A$. The conditions
on $F$ ensure $A\mapsto Prob(X\in A)$ is indeed a probabilty measure.

Two random variables, $X$, $Y$, are completely described by their 
{\em joint distribution}, $F(x, y) = Prob(X\le x, Y\le y)$. In this
case the properties required for a function to be the joint distribution
of two random variables start to get more complicate. The modern approach
defines random variables to be measurable functions on a 
probability space. 
If $X_1$, $X_2$, \dots are functions on a probabilty space we
can define $Prob(X_1\le x_1, X_2\le x_2, \dots) = P(\{\omega\in\Omega:
X_1(\omega)\le x_1, X_2(\omega)\le x_2, \dots\})$.

\section{Stochastic Processes}
\label{pt-sp}
A {\em stochastic process} is a sequence of random variables indexed by time.
It is specified by a probability space, $(\Omega, P, \F)$, an increasing sequence
of $\sigma$-algebras $(\F_t)_{t\ge0}$, and a collection 
of random variables, $(X_t)_{t\ge0}$,
such that $X_t$ is $\F_t$ measurable for $t\ge0$. More generally, we can
replace $t\ge0$ by any totally ordered set.

A {\em martingale} is a stochastic process, $X_t$, such that $E[X_u|\F_t] = X_t$
wherenever $t \le u$. A process has {\em independent increments} if
$X_u - X_t$ is independent of $\F_t$ whenever $t\le u$. A process is
{\em stationary} if the law of $X_{t_1} - X_{t_0}$, \dots, $X_{t_n} - X_{t_{n-1}}$
is the same as the law of $X_{t_1 + s} - X_{t_0 + s}$, \dots, $X_{t_n + s} - X_{t_{n-1} + s}$
for all $s$, and $t_j$.

\section*{Notes}
\subsection{\ref{pt-intro}. Introduction}
Probability Theory had its origins in games of
chance and was not considered to be a part of mainstream mathematics until
fairly recently. It can be thought of as a generalization of the Propositional
Calculus for logic that involves combining statements (`propositions') that 
can be either `true' or `false' using the operations `and', `or', `not', etc.
George Boole \cite{Boole:1853} proposed a set of laws that propositions satisfy that
could be used to reduce (a very limited subset of) logical reasoning to algebraic 
manipulation. The notion human reasoning could be reduced to a mechanical computation
was quite stunning at the time. 

Probability theory deals with `events' that have a `probability' between zero 
and one. For events having probability zero or one, it reduces to the Propositional
Calculus. There is not uniform agreement on what `probability' means.  Is it
subjective or objective? Some schools interpret it as a frequency in order to give
it an objective meaning, but modern thinking is that this is too restrictive.
Bayesians consider it to be subjective and that it represents a
degree of belief conditional on available information. To reconcile the 
unsatisfactory notion of everyone having their own personal (`prior') probabilities, 
they claim that repeated application of Bayes' Theorem involving increasing
information would result in convergence of posterior probabilities.

% (\cite{Kolmogorov:1933},\cite{Kolmogorov:1950}).

%Is a probability a frequency? Is a probablity objective or subjective?

%Emile Borel - probability is subjective

%1923 Hausdorff - event is a measurable subset from a Borel field. (Stone's Theorem)

% complexity and probability

Schematically, given data $x_1$, $x_2$, \dots, and hypotheses $h_1$, $h_2$, \dots, one
wishes to determine which hypothesis was was most likely. Bayes' Theorem provides a
means of doing this if the hypotheses are assigned probabilities, but provides no
guidance on assigning these a priori probabilities. In 1964 Solomonoff \cite{Solomonoff:1964} 
obtained the remarkable result that there is a canonical prior distribution. 
This distribution is related to Kolomogorov complexity, computability, and artificial
intelligence \cite{AIT:2007}, but that is a fascinating story beyond the scope of this book.

\subsection{\ref{pt-ss}. Sample Spaces}
In probability theory,
all possible outcomes of a trial are modeled by a set called the {\em sample space}.
The possible results of flipping a coin can be described by the set $\{H, T\}$, where $H$ represents
the outcome being heads and $T$ represents tails. Likewise, the outcome of the roll of
a die is an element of the set $\{1,2,3,4,5,6\}$. Of course it is possible that the
coin miraculously ends up balancing on its edge or the police raid your craps table 
while the die is in mid roll. Mathematics is just an idealization of the real world.

An element of the sample space is an {\em outcome} that represents complete knowledge of what occurred.
A subset of the sample space is called an {\em event}. E.g., the `an even number was
rolled' in the die example is the subset $\{2,4,6\}$.

Recall the {\em cartesian product} of two sets is $A\times B = \{(a,b)\colon a\in A, b\in B\}$.
Its elements correspond to all ways of choosing one element from the first set and one
element from the second set.  We write $A^n$ for the cartesian product of the set $A$ with itself $n$ times.
It can be thought of as repeated trials of sampling from $A$. If we let $n = \{0, 1, \dots, n-1\}$,
then every function $a\colon n\to A$ corresponds to an element of $A^n$. In general
$A^B = \{f\colon A\to B\}$ is the set of all functions from the set $B$ to the set $A$.

\subsection{Boolean Algebras}
In what follows, we will limit ourselves to Boolean algebras with a finite number of
elements that can be represented as sets, where `and' corresponds to set intersection,
`or' to set union, and `not' to set complement. A collection of subsets of a
sample space is a
{\em Boolean algebra} if it is closed under intersection and complement. By De Morgan's
Laws, it is also closed under intersection. 
An event is an {\em atom} if it is not empty and no proper subset belongs to the algebra.
Recall a {\em partition} of a set is a collection of pairwise disjoint subsets sets with union
equal to the set.

\begin{lemma}
The atoms of a Boolean algebra form a partition.
\end{lemma}
\begin{proof}
For each point in the set, the intersection of all events containing that point is an atom,
so the union of all atoms is the entire set. If two atoms have a nonempty intersection,
the atoms must be equal, so the atoms are pairwise disjoint.
\end{proof}

The importance of Boolean algebras is that they represent partial information. If the
set represents all possible outcomes, knowing to which atom an outcome belongs provides
some information about the outcome. For example, the partition $\{\{1,3,5\},\{2,4,6\}\}$
represents the information 'we know whether the outcome was even or odd' for the
roll of a die example.

\subsection{Conditional Expectation}
Given two events, $A$ and $B$, define the {\em conditional probability}
of $A$ given $B$ by $P(A|B) = P(A\cap B)/P(B)$. This is the natural
way of localizing the probability measure to the set $B$. The
function $A\mapsto P(A|B)$ for $A\subset B$ is a probability measure.
($B$ is the new $\Omega$.) 
\begin{exercise}
Show $P(B|B) = 1$ and $P(A_1\cup A_2|B) = P(A_1|B) + P(A_2|B)$
if $A_1$ and $A_2$ are disjoint.
\end{exercise}

The general formula for the intersection of two events is $P(AB)
= P(A)P(B|A)$. (From here on we write $AB$ for $A\cap B$.)  Note 
$P(A|\Omega) = P(A)$ for all events $A$ so in some sense all 
probabilities are conditional.

From the definition of conditional probability, $P(A\cap B) = P(A)P(B|A)$. Since
$P(A\cap B) = P(B\cap A)$, we have $P(A)P(B|A) = P(B)P(A|B)$, hence
$P(A|B) = P(A)P(B|A)/P(B)$.
Note that if $A$ and $B$ are independent, then $P(A|B) = P(A)$ and
$P(B|A) = P(B)$. I.e., neither event provides additional information
about the other.

\begin{exercise}
Show $P(AB|C) = P(A|C) P(B|AC)$ for all events $A$, $B$, $C$.
\end{exercise}

