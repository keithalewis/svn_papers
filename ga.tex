\documentclass[fleqn]{amsart}
\usepackage{amssymb,amsmath,amsthm,hyperref}

\def\Var{\mathop{\rm Var}}
\def\Cov{\mathop{\rm Cov}}
\newcommand{\RR}{\mathbb{R}}

\newtheorem{definition}{Definition}[section]
%\newtheorem{theorem}[definition]{Theorem}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{example}{Example}

\title{Hermann Grassmann's Geometric Calculus}
\author{Keith A. Lewis}
\email{kal@kalx.net}
\address{KALX, LLC \tt{\url{http://kalx.net}}}

\begin{document}
\maketitle

\begin{section}{Introduction}

In 1844 Hermann Grassmann published {\em Die Lineale Ausdehnungslehre,
ein neuer Zweig der Mathematik}. It was a groundbreaking work that
demonstrated how to reduce geometric facts to algebraic computations.

Reducing human thought to calculation was the tenor of the time. George
Boole published his {\em Laws of Thought} in 1854. The complete title
is {\em An Investigation of the Laws of Thought on Which are Founded
the Mathematical Theories of Logic and Probabilities}. Grassmann's idea
suffered only from neglect, but Boole's notions about probability were
actively denounced. The most devastating critique was John Maynard Keynes
1921 book {\em A Treatise On Probability}.  Of course everyone promptly
forgot Keynes' theory of probability once Andrei Kolmogorov nailed the
modern version of the theory down in {\em Foundations of the Theory of
Probability} 1933.

I am a mathematician, not a psychologist, so I can only attempt to lay
out a clear explanation of Grassmann's ideas. It is a mystery to me
why others have not adopted them. They provide a simple way of reducing
geometric truths to straightforward calculations.

In modern terms, Grassmann's key idea was to take Euclidean space, $E$, and 
consider the (unital, associative) algebra (over the real numbers)
generated by it modulo the relation $P^2 = 0$ for any $P\in E$.

If $P$ and $Q$ are two points, $PQ$ is the 'line' they determine. If
$R$ is another point, $PQR$ is the 'plane' they determined. This is
Grassmann's {\em progressive product} and it need not stop at
three dimensions. It was a pretty heady notion in
his day to think about general $n$-dimensional space. There was still
a lot of intellectual brainpower at that time being spent on finding a
proof of Euclid's fifth axiom in plane geometry until Lobachevsky and
Bolyai put those guys out of business.

Perhaps one reason for the slow adoption of his calculus was the
difficulty of defining his {\em regressive product}. Once the number
of points become larger than the dimension of the space, what does the
product mean?

Grassmann clearly understood it meant the intersection of two higher
dimensional quantities, but the general definition took 100 years. Rota,
et al. We use Rota's definition but extend it from the space of
vectors to the entire Grassmann algebra.

\end{section}

\begin{section}{Points}
Euclidean space, $E$, is a set acted on by a vector space (over the
real numbers), $V$. The point $P\in E$, is sent to $P + v$ by the vector
$v\in V$.  This action is related to the vector space addition by $(P +
v) + w = P + (v + w)$.  We further assume that if $P + v = Q + v$ then
$P = Q$ and for every $P, Q\in E$ there exists a unique vector $v\in V$
such that $P + v = Q$. We write $v = Q - P$.

There is a third type of addition making the set of weighted points
$\RR\times E$ an affine space. Writing $tP$ for the pair $(t, P)$ define
$(1 - t)P + tQ$ by the vector space action $P + t(Q - P)$.  In general $uP
+ vQ = (u + v)R$ where $R = (1 - t)P + tQ$ and $t = v/(u + v)$. Note $Q -
P$ is not a point in this space.

A finite set of points are {\em linearly dependent} if there exist
weights, not all zero, such that the weighted linear combination of
points is 0.  Grassmann's astounding intuition was that the smallest
(associative) algebra (over the real numbers) modulo the relation that a
product of linearly dependent points is zero can reduce geometric truths
to a straightforward calculation.

Since $P$, $Q$ are linearly dependent if and only if $P = Q$,
Grassmann's assumption implies $P Q = 0$ if and only if $P = Q$.
A simple consequence is that $PQ = -QP$ since
$0 = (P + Q)(P + Q)$.

Define $R(t) = R(t; P, Q) = (1 - t)P + tQ$.
If $P$, $Q$, and $R$ are linearly dependent,
and none of the points coincide, there exists $t$ with $R(t) = R$,
hence $PQR = PQ((1 - t)P + tQ) = (1 - t)PQP + t PQQ = 0$.
The converse holds by Grassman's assumption.

In Grassmann's interpretation, if $A$ and $B$ are points, then $A + B$
is the midpoint between $A$ and $B$ with weight 2.  If $A$, $B$, and $C$
are points, then $A + B + C$ is the barycenter of the triangle determined
by the points with weight 3. The medians of the triangle are $A(B + C)$,
$B(C + A)$ and $C(A + B)$.  Note $A(B + C)(A + B + C) = (AB + AC)(A +
B + C) = ABC + ACB = 0$.  This proves the medians of a triangle intersect
at the barycenter.

\end{section}

\begin{section}{Lines}

The vague notion is that $PQ$ is the `line' determined by the points $P$
and $Q$. We need to actually perform calculations on the mathematical
objects in the Grassmann algebra. The product of
two points is called a {\em 1-dimensional extensor}.

We begin our study of Grassmann's algebra by considering equality
of 1-dimensional extensors. To be uniform, we should call
points, oxymoronically, 0-dimensional extensors. As we have
already seen, equality of 0-dimensional extensors means
they are the same point.

\begin{lemma}
If $P,Q$ and $R,S$ are two pairs of points, then
$PQ = RS$ if and only if there exists $t$ such
that $R = P + t(Q - P)$ and $S = Q + t(Q - P)$.
\end{lemma}

\begin{proof}
In other words, the line segment determined by $RS$ is a
translation of $PQ$ along $PQ$. Since $PQR = RSR = 0$
there exists $t$ with $R = R(t)$.  Likewise there exists
$u$ with $S = R(u)$. We have $PQ = RS = R(t)R(u)
= (1 - t)uPQ + t(1 - u)QP = (u - tu - t + tu)PQ$ hence
$u - t = 1$.  This shows $S = R(u) = R(t + 1)
= P + (t + 1)(Q - P) = Q + t(Q - P)$.

\end{proof}

The lemma shows we can think of 1-de's as {\em weighted lines}.
Linear combinations of 1-de's are called 1-forms. The smallest linear
subspace containing the 0-de's is just the set of weighted points in
Euclidean space.

\begin{subsection}{Scalar Multiples}

If $tPQ = RS$ for some scalar $t$ we write $t = RS/PQ$ and vice
versa. Note that with $R(t)$ as defined above $PR(t)/PQ > 0$ if and only
if $t > 0$. This shows $Q$ and $R$ are on the same side of $P$
if and only if $PR/PQ > 0$. Similarly we have $R$ lies between
$P$ and $Q$ if and only if $0 < t < 1$. Since
$R(t)Q/PQ = 1 - t$ we have $R(t)$ lies between $P$ and $Q$
if and only if ${\bf R}Q/PQ > 0$ and $P{\bf R}/PQ > 0$.

We can also now drop the $t$ and simply write
$R = RQ/PQ P + PR/PQ Q$ for any point $R$ on the line
determined by $P$ and $Q$.

\end{subsection}

Unlike for points, the space of 1-forms is not in general the same as
the set of 1-de's.

\begin{lemma}
If $E$ has dimension $n$, the space of 1-forms has
dimension $\binom{n + 1}{2}$.
\end{lemma}

\begin{proof}
Recall $\binom{n}{k}$ is the number of ways to choose $k$
elements from a set of $n$ elements.  Choose an arbitrary
point $P_0\in E$ (since there is no $0\in E$) and let
$v_1$, \dots, $v_n$ be a basis of $V$, the $n$-dimensional
vector space acting on $E$. Define $P_j = P_0 + v_j$.
Every point in $E$ can be expressed as a linear
combination of $P_0$, \dots, $P_n$ so
$P_iP_j$, $0\le i < j\le n$ form a basis for the 1-forms.
\end{proof}

\begin{subsection}{Regressive Product}

Define the {\em regressive product} of the 1-de's $AB$ and $CD$
by $AB.CD = BCD.A - ACD.B$ or 0 if the lines do not intersect.
Clearly the regressive product is on the line containing
$A$ and $B$. If we multiply by $CD$ on the right we get
$BCD.ACD - ACD.BCD$, so if $BCD.ACD = ACD.BCD$ this
would show the regressive product is on the line $CD$.
If the products of three points were scalars, this would 
clearly be true. They are not scalars, but as we will
see in the next section on Planes, they act very much like them.

We will give proofs in the next section, but let's not let that stop
us from giving another proof that the medians of a triangle meet at
a point: $A(B + C).B(C + A) = (B + C)B(C + A).A - AB(C + A).(B + C) =
CB(C + A).A - ABC.(B + C) = CBA.A - ABC.(B + C) = CBA.(A + B + C)$.

Note that this time around we did not have to guess that $A + B + C$
was the common point of intersection, the regressive product
gave us the answer.

\end{subsection}

\end{section}

\begin{section}{Planes}

Next we consider the 2-dimensional extensors that determine a plane
formed by the product of three points. Given $P$, $Q$, and $R$
such that $PQR \not=0$, define $S(t, u) = P + t(Q - P) + u(R - P)
= (1 - t - u)P + tQ + u R$. If $PQR = STU$ then $S$, $T$, and
$R$ all lie in the plane determined by $P$, $Q$, and $R$.
Assume for the moment that $S = P$, then we can find scalars
$t$, $u$, and $v$, $w$ such that
$T = S(t, u)$ and $U = S(v, w)$. We find $STU = PQR$ if
and only if $tw - uv = 1$.

Note $PQS(t, u) = uPQR$ so $PQS/PQR > 0$ if and only if
$R$ and $S$ are on the same side of the line $PQ$. Repeating
this two more times shows the point $S$ is in the triangle
determined by $PQR$ if and only if ${\bf S}QR/PQR > 0$, $P{\bf S}R/PQR > 0$,
and $PQ{\bf S}/PQR > 0$.

\end{section}

\begin{section}{Volume}

Consider the 3-dimensional extensors that determine a volume
formed by the product of 4 points. Suppose $PQRS = TUVW$ and
is nonzero. Consider the linear transformation $A\colon E\to E$
determined by $AP = T$, $AQ = U$, $AR = V$, and $AS = W$.
As before $TUVW = aPQRS$ for some scalar $a$. This
scalar is defined to be the determinant of the linear
transformation $A$.

Similar to the previous sections $PQRT/PQRS > 0$ if and only
if $S$ and $T$ are on the same side of the plane $PQR$.
Likewise,
$T$ is in the tetrahedron determined by $PQRS$ if and only
if ${\bf T}QRS/PQRS > 0$, $P{\bf T}RS/PQRS > 0$, $PQ{\bf T}S/PQRS
> 0$, and $PQR{\bf T}/PQRS > 0$.

\end{section}

\end{document}
